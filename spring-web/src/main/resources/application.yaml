server:
  tomcat:
    uri-encoding: UTF-8
  servlet:
    encoding:
      charset: UTF-8
      force: true
      enabled: true

spring:
  banner:
    location: banner.txt
  profiles:
    active: ${ENV_TYPE:dev}
  mvc:
    async:
      request-timeout: 600000
  main:
    allow-bean-definition-overriding: true
  servlet:
    multipart:
      # 单个文件大小
      max-file-size: 100MB
      # 总上传文件大小
      max-request-size: 500MB
  jackson:
    time-zone: GMT+8
    date-format: yyyy-MM-dd HH:mm:ss
  swagger:
    groupName: spring-web
    basePackage: com.self
    version: 1.0.0
    title: spring-web
    description: spring-web 开发文档
    contactName: zp
    contactEmail: default@default.com
    contactUrl: www.default.com
  quartz:
    job-store-type: jdbc
    jdbc:
      initialize-schema: never
    wait-for-jobs-to-complete-on-shutdown: true
    properties:
      org:
        quartz:
          scheduler:
            instanceId: AUTO
            instanceName: quartzScheduler
          jobStore:
            class: org.springframework.scheduling.quartz.LocalDataSourceJobStore
            driverDelegateClass: org.quartz.impl.jdbcjobstore.StdJDBCDelegate
            useProperties: false
            misfireThreshold: 12000
            tablePrefix: QRTZ_
            isClustered: true
            clusterCheckinInterval: 10000
            maxMisfiresToHandleAtATime: 1
            txIsolationLevelSerializable: true
          threadPool:
            class: org.quartz.simpl.SimpleThreadPool
            threadCount: 20
            threadPriority: 5
            threadsInheritContextClassLoaderOfInitializingThread: true
  kafka:
    # 副本数
    streams:
      replication-factor: 3
    # kafka producer
    producer:
      # 写入失败时，重试次数。当leader节点失效，一个repli节点会替代成为leader节点，此时可能出现写入失败，
      # 当retris为0时，produce不会重复。retirs重发，此时repli节点完全成为leader节点，不会产生消息丢失。
      retries: 0
      # 每次批量发送消息的数量,produce积累到一定数据，一次发送
      batch-size: 0
      # produce积累数据一次发送，缓存大小达到buffer.memory就发送数据
      buffer-memory: 1024000
      # procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：
      # acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。
      # acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
      # acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。
      # 可以设置的值为：all, -1, 0, 1
      acks: 1
      # 指定消息key和消息体的编解码方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer

    # kafka consumer
    consumer:
      # smallest和largest才有效，如果smallest重新0开始读取，如果是largest从logfile的offset读取。一般情况下我们都是设置smallest
      auto-offset-reset: latest
      # enable.auto.commit:true --> 设置自动提交offset
      enable-auto-commit: false
      # 如果'enable.auto.commit'为true，则消费者偏移自动提交给Kafka的频率（以毫秒为单位），默认值为5000
      auto-commit-interval: 3000
      # 指定消息key和消息体的编解码方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    listener:
      # 监听主题不存在时不会报错
      missing-topics-fatal: false
      # 手动提交模式
      ack-mode: manual_immediate

mybatis-plus:
  mapper-locations: classpath*:mapper/**/*.xml

logging:
  config: ${LOGING_CONFIG:classpath:logback/logback-test.xml}
  level:
    root: info